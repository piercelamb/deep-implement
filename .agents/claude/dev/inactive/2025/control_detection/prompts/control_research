ultrathink

we are going to plan a feature. For this feature, you will write a script. The script will iterate the pdf files in `template_polices` and for each file you will:

- upload the PDF into the gemini context cache for the highest possible model (gemini 3 preferred, 2.5 pro 3 doesnt have it)
- for each ground truth DCF you will submit a LLM-request (prompt). The prompt will amount to asking the LLM to enumerate reasons that the control is mapped to this policy. It should cite page numbers where evidence can be found
- Recall that the prompt needs to load from disk as separate system/user/response.json files (response is the structured output file) and have runtime context insert into it via placeholders. (refer to @.agents/claude/dev/active/control_detection/planning/llm_control_detection/llm_control_detection.md to learn more about this style of loading prompts)
- The script will write these reasons to the correct parsed_policies dir in here: @ai_services/scripts/experiments/control_detection/files/experiments/template_policies/parsed_policies
- It'd be nice to have a single .md file per policy that just gets appended to as each prompt returns reasons for the controls mapping.
- Once every control has completed for a given policy, you will delete the pdf from the context cache

You will iterate N policies in parallel and perform this task. N needs to be configurable.
You should build it in a way where it's easy to can run it with 1 file with 1 ground truth DCF while we test it
You should use a test-driven-development (TDD) approach