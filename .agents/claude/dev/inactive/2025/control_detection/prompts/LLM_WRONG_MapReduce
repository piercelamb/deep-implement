ultrathink

so we wrote this feature to extract explanations for why ground truth DCFs mapped to polices:
@.agents/claude/dev/active/control_detection/planning/ground_truth_explanations/plan.md

We then wrote this feature to take those explanations and use a MapReduce process to merge similar explanations and track unique
explanations: @.agents/claude/dev/active/control_detection/planning/map_reduce/plan.md

Once complete, we used these merged/rare rules to create a set of instructions for how to map policies to controls

in @ai_services/scripts/experiments/control_detection/files/llm_outputs/gt_validation/20251227_145838/detailed_results.json

we have analyed all of the times the LLM incorrectly didnt not produce a ground truth control mapping across our 37 documents. These are
the LLM_WRONG entries. We have a bunch of reasoning/evidence on why the LLM is wrong for each entry.

I'm curious if we could use the same MapReduce approach we used to create instructions for how to map policies to controls to create
instructions that save the LLM from making these false negative mistakes on ground truth controls

Would it be as simple as writing a new prompt or two and just calling it in a branch in the MapReduce code
(@ai_services/scripts/experiments/control_detection/reason_aggregator)?

Do some research and try to understand the easiest path that shares the most code to iterating the LLM_WRONG entries and doing the
MapReduce approach to distill merged and rare rules from those entries

Write a research/plan document to @.agents/claude/dev/active/control_detection/planning/research/LLM_WRONG_MapReduce