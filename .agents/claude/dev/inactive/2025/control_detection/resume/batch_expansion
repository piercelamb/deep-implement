> ultrathink

this is looking great. For production we want to trim LLM calls in some way and probably reach MAX_BATCH_SIZE so we have fewer LLM calls.

For this experiment, we want to always hit all MAX_CALLS_PER_DOCUMENT LLM calls (unless we return under MAX_CALLS_PER_DOCUMENT controls).
 This will ensure we always give the LLM the least amount context and establish a baseline for the experiment.

so, for e.g. in the under capacity logs:

INFO - ColModernVBERT: 450 controls × 25 pages = 11250 scores, 823 passed threshold (0.25), 25 pages with matches
  INFO - Dedupe: 25 pages, 823 scored entries → 200 unique controls
  INFO - Page grouping: 200 controls → 25 page groups (0 oversized)
  INFO - After splitting: 25 batches (from 25 page groups)
  INFO - No consolidation needed: 25 batches <= 50 max_calls
  INFO - Created 25 page-aware batches (size: min=4, max=10, avg=8.0), starting LLM calls...

We should always be hitting 50 max calls (if we have 50 or more controls) with the smallest possible batch size while taking into account
 page grouping and k-means grouping

How can we do that?

⏺ Great question. Currently we only consolidate (merge batches when over MAX_CALLS), but we never expand (split batches when under MAX_CALLS).

  For the experiment, we need to add an expansion step:

  Current vs Desired Behavior

  Current (200 controls):
    Page grouping → 25 batches
    No consolidation (25 < 50)
    Result: 25 batches, avg 8 controls each

  Desired (200 controls):
    Page grouping → 25 batches
    EXPAND to 50 batches (split largest batches)
    Result: 50 batches, avg 4 controls each

  Proposed Algorithm: expand_batches()

  def expand_batches(
      batches: list[PageAwareControlBatch],
      target_count: int,
  ) -> list[PageAwareControlBatch]:
      """
      Expand batches by splitting largest ones until we reach target_count.

      Preserves page grouping: when a batch is split, both halves retain
      the same source_pages (they came from the same page group).

      Algorithm:
          While len(batches) < target_count:
              1. Find largest batch with size > 1
              2. Split it in half (preserving source_pages)
              3. Replace original with two halves
      """
      result = list(batches)

      while len(result) < target_count:
          # Find largest splittable batch
          splittable = [b for b in result if len(b) > 1]
          if not splittable:
              break  # Can't split further (all batches have 1 control)

          largest = max(splittable, key=len)
          result.remove(largest)

          # Split in half, preserving source_pages
          mid = len(largest.controls) // 2
          controls = largest.controls
          source_pages = largest.source_pages

          batch1 = PageAwareControlBatch(
              controls=controls[:mid],
              source_pages=source_pages[:mid],
          )
          batch2 = PageAwareControlBatch(
              controls=controls[mid:],
              source_pages=source_pages[mid:],
          )

          result.extend([batch1, batch2])

      return sorted(result, key=lambda b: b.primary_page)

  Updated Flow

  # In create_page_aware_batches():

  # Step 3a: Consolidate if over max_calls
  if len(batches) > max_calls:
      batches = consolidate_batches(batches, max_calls, max_batch_size)

  # Step 3b: Expand if under max_calls (experiment mode - minimize batch sizes)
  elif len(batches) < max_calls and len(candidates) >= max_calls:
      batches = expand_batches(batches, target_count=max_calls)

  New Example Logs

  INFO - ColModernVBERT: 450 controls × 25 pages = 11250 scores, 823 passed threshold (0.25), 25 pages with matches
  INFO - Dedupe: 25 pages, 823 scored entries → 200 unique controls
  INFO - Page grouping: 200 controls → 25 page groups (0 oversized)
  INFO - After splitting: 25 batches (from 25 page groups)
  INFO - Expansion: 25 → 50 batches (target: 50, minimizing batch sizes)
  INFO - Created 50 page-aware batches (size: min=2, max=6, avg=4.0), starting LLM calls...

  Questions Before Implementation

  1. Should this be configurable? Add a flag like minimize_batch_sizes: bool = True for experiment mode vs False for production?
  2. Deterministic splitting? When splitting, should we sort controls by score or control_id first to ensure determinism?
  3. Tie-breaking for "largest batch"? If multiple batches have the same size, which one do we split first? (I'd suggest lowest primary_page for determinism)

  Want me to implement this?

  - yes lets make it configurable in @ai_services/scripts/experiments/control_detection/experiment_config.py. Configured once and imported
  everywhere

  - lets sort by control_id

  - Lowest primary page for tie breaking splitting

  Create a todo list and start implementing