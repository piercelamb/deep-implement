  ======================================================================
  EXPERIMENT RESULTS
  ======================================================================
  Documents evaluated: 1

  Retrieval Stage (before LLM):
    Embedding recall: 100.0% (8/8 GT controls pass threshold)
    Sent to LLM:      100.0% (8/8 GT controls in LLM batches)

  Counts:
    Ground truth controls: 8
    Predicted controls:    84
    True positives:        6
    False positives:       78
    False negatives:       2

  Micro-averaged (pooled across all docs):
    Precision: 7.1%
    Recall:    75.0%
    F1:        13.0%

  Macro-averaged (mean of per-doc metrics):
    Precision: 7.1%
    Recall:    75.0%
    F1:        13.0%
  ======================================================================

  Make sure that `Sent to LLM` is calculated AFTER all the deduping/filtering/clustering/batching code so we know exactly how the code
  effects recall RIGHT BEFORE the LLM sees the controls