⏺ Experiment Comparison

  | Metric           | Exp 1 (Page-centric)  | Exp 2 (Page-centric +) | Exp 3 (Control-centric) |
  |------------------|-----------------------|------------------------|-------------------------|
  | Date             | Dec 23                | Dec 23                 | Dec 27                  |
  | Mode             | Page-centric (top-50) | Page-centric (top-100) | Control-centric         |
  | Documents        | 37                    | 35                     | 35                      |
  | GT Controls      | 686                   | 512                    | 620                     |
  | Embedding Recall | 95.3%                 | 93.8%                  | 95.5%                   |
  | TopK/Sent Recall | 77.0%                 | 81.6%                  | 93.9%                   |
  | LLM Recall       | 34.3%                 | 30.5%                  | 73.9%                   |
  | LLM Precision    | 42.0%                 | 36.4%                  | 17.4%                   |
  | F1               | 37.7%                 | 33.2%                  | 28.2%                   |
  | True Positives   | 235                   | 156                    | 458                     |
  | False Positives  | 325                   | 273                    | 2171                    |
  | False Negatives  | 451                   | 356                    | 162                     |

  ---
  Conclusions

  1. Control-centric mode dramatically improved recall

  The shift from page-centric to control-centric more than doubled LLM recall (34% → 74%). This is because:
  - Controls are now sent to the LLM more reliably (93.9% vs 77%)
  - The LLM sees the full document with cached PDF, not just single pages
  - Controls get dedicated evaluation attention

  2. But precision collapsed

  Precision dropped from ~40% to 17%. The system predicted 2,629 controls but only 458 were in ground truth - meaning 2,171 false positives.

  3. The trade-off is asymmetric

  - Page-centric: Conservative - misses many controls (451 FN) but makes fewer false claims (325 FP)
  - Control-centric: Aggressive - catches most controls (only 162 FN) but over-predicts massively (2,171 FP)

  4. Critical question: Is ground truth wrong?

  This is where the GT validation pipeline becomes essential. The 2,171 "false positives" fall into one of two categories:
  1. LLM is wrong: The control genuinely doesn't map to the policy
  2. GT is wrong: The control actually DOES map, but wasn't labeled in ground truth

  Given that the ground truth was manually curated and likely conservative, some portion of those 2,171 FPs could be legitimate mappings that humans missed.

  5. Next step recommendation

  Run the GT validation judge on experiment 3's results to:
  - Evaluate the 162 false negatives (GT says yes, LLM says no) - judge will likely say LLM_WRONG for most
  - Sample the 2,171 false positives (LLM says yes, GT says no) - if judge says GT_WRONG frequently, the ground truth needs expansion

  The control-centric approach appears to have the right instinct (high recall), but may need prompt tuning to improve precision without sacrificing recall.

  -----

  GT Validation Results Summary

  Run Configuration

  - Model: gemini-3-pro-preview
  - Experiment timestamp: 20251227_145838 (same as Experiment 3)
  - Max concurrent judges: 5

  ---
  High-Level Verdicts

  | Verdict   | Count | Percentage | Meaning                                         |
  |-----------|-------|------------|-------------------------------------------------|
  | LLM_WRONG | 146   | 59%        | LLM missed it - Ground Truth is correct         |
  | GT_WRONG  | 101   | 41%        | Ground Truth is wrong - LLM was right to reject |
  | UNCERTAIN | 0     | 0%         | Judge couldn't decide                           |

  By Dispute Reason (why the control was disputed)

  | Reason   | Count | Description                                     |
  |----------|-------|-------------------------------------------------|
  | NO_MATCH | 106   | LLM explicitly said "doesn't map"               |
  | NOT_SENT | 104   | Control never reached LLM (retrieval filtering) |
  | PARTIAL  | 37    | LLM said "partial match, not full MAPPED"       |

  ---
  Key Insights

  1. Ground Truth has significant errors (~41%)

  Of the 247 disputed controls judged, 101 were incorrectly labeled in GT. This confirms the hypothesis that the ground truth is incomplete/incorrect in many cases.

  2. Retrieval is dropping valid GT controls

  104 controls were NOT_SENT to the LLM. Of these, many are judged LLM_WRONG, meaning:
  - The retrieval stage (ColModernVBERT threshold) is filtering out valid GT controls
  - These controls should have been sent but scored below the 0.48 threshold

  3. PARTIAL decisions are mostly correct GT

  Of the 37 PARTIAL disputes, most were judged LLM_WRONG, meaning:
  - The LLM was being too strict by not calling these MAPPED
  - Policy-level mandates (even without exact frequency) should often count as MAPPED

  ---
  Policy-by-Policy Breakdown

  | Policy                         | Judged | LLM_WRONG | GT_WRONG | Pattern                        |
  |--------------------------------|--------|-----------|----------|--------------------------------|
  | Change Management Policy       | 25     | 5         | 20       | GT heavily over-labeled        |
  | AIMS Plan                      | 22     | 20        | 2        | Retrieval issue (all NOT_SENT) |
  | Asset Management Policy        | 22     | 12        | 10       | Mixed                          |
  | Data Protection Policy         | 19     | 9         | 10       | Mixed                          |
  | ISMS/PIMS Plan                 | 15     | 15        | 0        | Retrieval issue (all NOT_SENT) |
  | ISMS Plan 2022                 | 17     | 17        | 0        | Retrieval issue (all NOT_SENT) |
  | PCI DSS Compliance             | 16     | 13        | 3        | LLM too strict                 |
  | Logging and Monitoring         | 11     | 5         | 6        | Mixed                          |
  | Risk Assessment Policy         | 10     | 5         | 5        | Split 50/50                    |
  | Physical Security Policy       | 9      | 8         | 1        | LLM too strict                 |
  | Information Security Policy    | 8      | 4         | 4        | Split 50/50                    |
  | Vulnerability Management       | 8      | 4         | 4        | Split 50/50                    |
  | Network Security Policy        | 7      | 1         | 6        | GT over-labeled                |
  | Software Development Lifecycle | 5      | 3         | 2        | Mixed                          |
  | Vendor Management Policy       | 5      | 3         | 2        | Mixed                          |
  | Password Policy                | 5      | 1         | 4        | GT over-labeled                |
  | Backup Policy                  | 4      | 0         | 4        | GT fully wrong                 |
  | Disaster Recovery Plan         | 4      | 2         | 2        | Split                          |
  | Incident Response Plan         | 4      | 3         | 1        | LLM too strict                 |
  | Data Classification Policy     | 3      | 1         | 2        | Mixed                          |
  | Data Retention Policy          | 3      | 0         | 3        | GT fully wrong                 |
  | System Security Planning       | 3      | 0         | 3        | GT fully wrong                 |
  | Other policies                 | 26     | 18        | 8        | Various                        |

  ---
  Conclusions

  What this means for Experiment 3's metrics:

  1. The 2,171 false positives aren't all wrong
    - We validated 247 of the 162 false negatives (GT controls the LLM missed)
    - 41% of those GT labels were actually incorrect
    - If this ratio holds for false positives, many "FPs" may be correct predictions
  2. Three distinct problem categories:
    - Retrieval gaps: ISMS/AIMS policies losing GT controls before LLM sees them (fix: lower threshold or improve embeddings)
    - LLM too strict: PARTIAL decisions for valid mappings (fix: prompt tuning)
    - GT over-labeled: Change Management, Network Security, Backup policies have wrong GT labels
  3. Ground truth needs cleanup:
    - 101 controls should be removed from GT across these policies
    - Especially: Change Management (20), Network Security (6), Backup (4), Data Retention (3)
  4. Next steps:
    - Apply GT corrections and re-run experiment
    - Sample false positives to validate in reverse (are LLM's extra predictions correct?)
    - Consider lowering retrieval threshold for ISMS-type documents

------

GT_WRONG Control Distribution

  | Metric                            | Count |
  |-----------------------------------|-------|
  | Total GT_WRONG verdicts           | 101   |
  | Unique control IDs                | 84    |
  | Controls appearing multiple times | 4     |
  | Controls appearing only once      | 80    |

  ---
  The Big Outlier: DCF-32 "Security Policies"

  DCF-32 appears 15 times - all judged GT_WRONG across these policies:

  - Asset Management Policy
  - Backup Policy
  - Business Continuity Plan
  - Data Classification Policy
  - Data Protection Policy
  - Data Retention Policy
  - Disaster Recovery Plan
  - Encryption Policy
  - Logging and Monitoring Policy
  - Network Security Policy
  - Password Policy
  - Risk Assessment Policy
  - Software Development Life Cycle Policy
  - Vendor Management Policy
  - Vulnerability Management Policy

  What DCF-32 requires:

  "Company policies are accessible to all employees and, as applicable, third parties. Personnel are required to acknowledge the information security policy and other topic-specific policies during onboarding and annually thereafter."

  Why it's GT_WRONG everywhere:

  This control is about having a policy acknowledgment process - not about the content of any individual policy. It should map to:
  - An Information Security Policy (the umbrella policy)
  - A Policy Management document
  - An Employee Handbook with acknowledgment requirements

  It should NOT map to topic-specific policies like "Backup Policy" or "Data Retention Policy" - those policies don't mandate their own acknowledgment.

  This is a systematic GT labeling error - someone likely tagged DCF-32 to every policy document in the dataset.

  ---
  Other Repeat Offenders (minor)

  | Control ID | Count | Name                         | Policies                  |
  |------------|-------|------------------------------|---------------------------|
  | DCF-27     | 2     | Cloud Resources Availability | Backup, Disaster Recovery |
  | DCF-78     | 2     | Storage Bucket Versioning    | Backup, Data Protection   |
  | DCF-58     | 2     | Centralized Authentication   | Data Protection, Password |

  These are borderline cases - potentially valid scope overlaps rather than systematic errors.

  ---
  Conclusion

  The GT errors are mostly unique/policy-specific (80 of 84 controls appear only once), except for:

  - DCF-32 accounts for 15% of all GT_WRONG verdicts (15/101)
  - Removing DCF-32 from those 15 policies would immediately improve GT quality

  The remaining 86 GT_WRONG verdicts are distributed across 83 unique controls, suggesting case-by-case labeling mistakes rather than systematic patterns.

======================================================================
FEEDBACK FOR GRC TEAM: Ground Truth Labeling Issues
======================================================================

Hi team,

We ran an automated validation pipeline using an LLM judge (Gemini Pro) to review 247 disputed control-to-policy mappings from the ground truth dataset. The judge evaluated each mapping against the actual policy document content and determined whether the GT label was correct.

**Bottom line: 41% of the reviewed GT labels (101 of 247) appear to be incorrect.**

Below is a summary of the key issues and recommendations.

---
## 1. Systematic Error: DCF-32 "Security Policies" (15 incorrect labels)

**The problem:** DCF-32 was tagged to 15 different topic-specific policies (Backup Policy, Data Retention Policy, Encryption Policy, etc.), but it should NOT map to any of them.

**What DCF-32 actually requires:**
> "Company policies are accessible to all employees... Personnel are required to acknowledge the information security policy and other topic-specific policies during onboarding and annually thereafter."

**Why these mappings are wrong:** This control is about having a *policy acknowledgment process* - it asks "Do employees sign off on policies?" It does NOT ask "Does this policy exist?" or "Does this policy contain security content?"

A Backup Policy doesn't mandate that employees acknowledge policies. It mandates backup procedures. These are completely different requirements.

**Correct mappings for DCF-32 would be:**
- Information Security Policy (if it mandates acknowledgment)
- Employee Handbook (if it requires policy sign-off)
- Policy Management Policy (if one exists)
- HR Onboarding documentation

**Action:** Remove DCF-32 from all 15 topic-specific policies listed above.

---
## 2. Change Management Policy: Over-labeled with Technical Controls (20 incorrect)

**The problem:** The Change Management Policy was tagged with Windows-specific hardening controls that have nothing to do with change management processes:

- DCF-966: Remote Credential Guard
- DCF-964: Local Security Authority (LSA) protection
- DCF-965: Credential Guard
- DCF-973: Attack Surface Reduction rules
- DCF-974/975/976/977: Microsoft Office Macro settings
- DCF-978: Web Browser Configuration
- DCF-979: OLE Blocking
- DCF-980: Office Productivity Suite Security
- DCF-981: PDF Software Security
- DCF-984: PowerShell Constrained Language Mode
- DCF-988: Internet Explorer 11 Disabled

**Why these mappings are wrong:** A Change Management Policy governs the *process* of making changes (approvals, testing, documentation). It does NOT define *what* configurations systems should have. These technical hardening controls belong in:
- System Hardening Policy
- Endpoint Security Policy
- Windows Configuration Baseline
- CIS Benchmark documentation

**The confusion:** Perhaps the thinking was "we need change management to implement these controls" - but that's backwards. The control asks "Is this configuration mandated?" not "How would we deploy it?"

**Action:** Remove all Windows-specific technical controls from Change Management Policy.

---
## 3. Policies with 100% GT_WRONG Rate

These policies had ALL of their disputed GT labels marked incorrect:

| Policy | GT_WRONG | Issue |
|--------|----------|-------|
| Backup Policy | 4/4 | Tagged with controls outside scope (versioning, HA architecture, restore testing, DCF-32) |
| Data Retention Policy | 3/3 | Tagged with disposal methods and CUI markings that aren't in the policy |
| System Security Planning Policy | 3/3 | Tagged with technical controls (external connections, flow control) that belong elsewhere |

**Action:** Review these policies' GT labels completely - the labeling approach may have been flawed.

---
## 4. Common Labeling Mistakes to Avoid

Based on the 101 incorrect labels, here are patterns to watch for:

### A. "Keyword matching" instead of "mandate matching"
❌ Wrong: "The policy mentions 'backup' so it maps to backup controls"
✅ Right: "The policy MANDATES that backups must be performed, so it maps to backup controls"

Example: A Disaster Recovery Plan that says "we will restore from backups" does NOT mandate DCF-78 (Storage Bucket Versioning). It assumes backups exist but doesn't require versioning.

### B. Confusing policy TYPES
- **Process policies** (Change Management, Incident Response) → map to process controls
- **Technical policies** (Hardening, Encryption) → map to technical controls
- **Topic policies** (Backup, Data Retention) → map to that topic's controls only

A Change Management Policy should NOT map to "Enable Credential Guard" - that's a technical control.

### C. Confusing "mentioned" vs "mandated"
❌ Wrong: "The policy lists training as a risk factor, so it maps to training controls"
✅ Right: "The policy REQUIRES that training programs be established, so it maps to training controls"

Example: AI Risk Management Policy *mentions* training as a context factor but doesn't *mandate* it - so it shouldn't map to DCF-804 (AI Training).

### D. Scope creep
Each policy has a defined scope. Don't tag controls outside that scope just because they seem related.

Example: Asset Management Policy manages IT assets (laptops, servers). It should NOT map to:
- DCF-818 (Physical Access Devices like keys/locks) - different asset type
- DCF-87 (Threat Detection System) - that's network security, not asset management

---
## 5. Recommended Actions

1. **Immediate:** Remove DCF-32 from all 15 topic-specific policies
2. **Immediate:** Remove all Windows hardening controls from Change Management Policy
3. **Review:** Audit Backup Policy, Data Retention Policy, and System Security Planning Policy labels
4. **Process:** When labeling, ask:
   - Does this policy contain a BINDING MANDATE (will/shall/must) for this control?
   - Is this control within the policy's SCOPE?
   - Would an auditor accept this policy as evidence for this control?

---
## 6. The Good News

59% of the disputed labels (146/247) were confirmed correct by the judge. The GT dataset isn't broken - it just needs targeted cleanup. The issues are concentrated in:
- One systematic error (DCF-32)
- One over-labeled policy (Change Management)
- A handful of scope-confusion cases

Fixing these would significantly improve GT quality.

---

Happy to discuss any of these findings or provide the detailed CSV with all 101 GT_WRONG judgments and reasoning.

— AI Team