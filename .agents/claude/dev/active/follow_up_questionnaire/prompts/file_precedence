We have a product called VRM Agent. This product attempts to automate vendor assessments using AI. It takes an organizations questionnaires as input and converts them into "criteria" (to-be-assessed per vendor), these criteria are then converted into targeted `criterion_question`s optimized for RAG. Then when an assessment is created, the organization uploads the vendors security documents (policies, SOC2, etc) and the criteria are assessed against the documents. These documents hit a `create_document` endpoint where they are indexed into a vectorDB namespace before the assessment starts.

The output of each criterion_question RAG run is "MET/NOT_MET/INCONCLUSIVE". The user can review these outputs and when they see a NOT_MET or INCONCLUSIVE, maybe they want the vendor to clarify or respond to something about it.

The VRM agent _automatically_ creates a follow up questionnaire combining all NOT_MET and INCONCLUSIVE criterion_questions for the vendor to clarify. The user has the chance to modify these criterion_questions before the follow up is sent or add new ones.

The vendor receives the questionnaire, when the vendor "submits", these follow up questionnaire values are stored in the database.

If the user DID NOT modify the automatic follow up questionnaire, we automatically re-run the assessment on _just_ the follow up questionnaire criterion_questions.

If the user DID modify the automatic follow up questionnaire, we automatically re-run the assessment on _all_ of the criterion_questions (since we can't know how their modifications impact the entire assessment)

The data entered into the follow-up questionnaire is supposed to be given precedence over any other data in the security review during these re-assessments. We already implemented a solution to handle this for *text-based* answers. Oversimplified, it works like this:

- Maintain a link between the original criterion_question's hash and the linked follow up question/answer, stored in vectorDB entry metadata
- Widen first stage retrieval to include more results
- If a follow up Q/A is in the first stage results and its criterion_question hash MATCHES the current criterion_question's hash, add it directly to the top_k results
- Thus it always ends up in the top_k given to the LLM.

However, a user can choose to add a question to the follow-up questionnaire that asks for a file upload. For e.g. "Please upload your data retention policy" where the vendor can directly upload a file.

These follow-up questionnaire file uploads will then hit the same `create_document` endpoint and land in the same vectorDB namespace as the originally uploaded documents.

Now we need a way to give these documents chunks "precedence" over other document chunks like we did with text-based answers.

Your task is to brainstorm ways we could do that
