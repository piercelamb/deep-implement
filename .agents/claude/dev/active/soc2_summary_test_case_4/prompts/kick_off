We have a product called VRM Agent. This product attempts to automate vendor assessments using AI. It takes an organizations questionnaires as input and converts them into "criteria" (to-be-assessed per vendor). Then when an assessment is created, the organization uploads the vendors security documents (policies, SOC2, etc) and the criteria are assessed against the documents.

The output of each criterion is "MET/NOT_MET/INCONCLUSIVE". The user can review these outputs and when they see a NOT_MET or INCONCLUSIVE, maybe they want the vendor to clarify or respond to something about it.

The endpoint that supports this assessment is called `start_vrm_vendor_assessment` in @/Users/plamb/Drata/ai-services/ai_services/api/routers/vrm_router.py

We have another endpoint in the same file called `start_soc2_summary_workflow` which runs a soc2 through an AI pipeline that produces some summary output. We have a flag in that endpoint that turns on indexing these summarized results to a vector DB. The passed vectorDB is supposed to be the same VectorDB supporting automatic vendor assessments above.

We have an eval that tests a single SOC2, indexed in a VRM agent vectorDB, running through start_vrm_vendor_assessment. Basically, given a vectorDB with this indexed SOC2 and a bunch of SOC2-related questions, here are some expected outputs.

The task is to come up with how we can run the soc2 summary endpoint such that it stores the summary outputs which allows us to run the eval with them included

Sketch out how you would do this
